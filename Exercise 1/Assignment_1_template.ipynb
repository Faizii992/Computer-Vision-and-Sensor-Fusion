{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Here you can find the necessary import"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dd1203e5790c147"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, confusion_matrix\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# you need the current working directory NB: works both windows and linux \n",
    "current_working_directory = os.getcwd()\n",
    "current_working_directory = os.path.dirname(current_working_directory)\n",
    "\n",
    "if not os.path.exists(f\"{current_working_directory}/Datasets\"):\n",
    "    os.makedirs(f\"{current_working_directory}/Datasets\")\n",
    "\n",
    "print(f\"[DATASET] PUT THE DATASET here: {current_working_directory}/Datasets\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df9527d568c5cc0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the directory where I want to download the dataset\n",
    "path_of_dataset = os.path.join(*['..', current_working_directory, 'Datasets', 'pizza_not_pizza'])\n",
    "print(f\"[DIR] The directory of the current dataset is {path_of_dataset}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9623622a470e2628"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data prep"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cae1c4d31600df3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# here let s do some functions that we can re-use also for other assignment\n",
    "def load_the_data_and_the_labels(data_set_path: str, target_size: tuple or None = None):\n",
    "    \"\"\"\n",
    "    This function help you to load the data dynamically \n",
    "    :param data_set_path: (str) put the path created in the previous cell (is the dataset path) \n",
    "    :param target_size: (tuple) the desired size of the images  \n",
    "    :return: \n",
    "        - array of images \n",
    "        - array with labels \n",
    "        - list of labels name (this is used for better visualization)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset, labels, name_of_the_labels = list(), list(), list() \n",
    "        # let s loop here and we try to discover how many class we have \n",
    "        for class_number, class_name in enumerate(os.listdir(data_set_path)):\n",
    "            full_path_the_data = os.path.join(*[data_set_path, class_name])\n",
    "            print(f\"[WALK] I am walking into {full_path_the_data}\")\n",
    "            \n",
    "            # add the list to nam _list\n",
    "            name_of_the_labels.append(class_name)\n",
    "            \n",
    "            for single_image in os.listdir(f\"{full_path_the_data}\"):\n",
    "                full_path_to_image = os.path.join(*[full_path_the_data, single_image])\n",
    "                \n",
    "                # add the class number \n",
    "                labels.append(class_number)\n",
    "                \n",
    "                if target_size is None:\n",
    "                    # let s load the image \n",
    "                    image = tf.keras.utils.load_img(full_path_to_image)\n",
    "                else:\n",
    "                    image = tf.keras.utils.load_img(full_path_to_image, target_size=target_size)\n",
    "                \n",
    "                # transform PIL object in image                    \n",
    "                image = tf.keras.utils.img_to_array(image)\n",
    "                \n",
    "                # add the image to the ds list \n",
    "                dataset.append(image)\n",
    "                \n",
    "        return np.array(dataset, dtype='uint8'), np.array(labels, dtype='int'), name_of_the_labels\n",
    "    except Exception as ex:\n",
    "        print(f\"[EXCEPTION] load the data and the labels throws exceptions {ex}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.925073386Z",
     "start_time": "2024-01-04T05:41:33.881226798Z"
    }
   },
   "id": "6f799e7da6302f9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the data "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e0aa3b23b7b6290"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# load the data "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.925265871Z",
     "start_time": "2024-01-04T05:41:33.924948422Z"
    }
   },
   "id": "f2b4e70f75e74c6b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Normalize the data "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca5a13bfe0a98bb7"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# normalize the data "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.926753600Z",
     "start_time": "2024-01-04T05:41:33.925186736Z"
    }
   },
   "id": "e115493bc0cb7b96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Split the data use the train_test_split function "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6dddfe71219ab4d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# split the data in train and test sets "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.928508516Z",
     "start_time": "2024-01-04T05:41:33.925331362Z"
    }
   },
   "id": "f347457af139c860"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the CNN according the instruction:\n",
    "        a. Input layer\n",
    "        b. Data augmentation, with random flip (horizontal and vertical) and random rotation (0.2).\n",
    "        c. Two hidden layers each composed with the following characteristics: 16 conv2d units, max pooling 2d and batch normalization, the second one should have 24 conv2d units max pooling 2d and batch normalization. \n",
    "        d. After this, add a flatten layer and a dense layer with 8 units\n",
    "        e. Add the final classifier (a  dense layer) with the correct number of output and activation\n",
    "        \n",
    "![alt text](assignment_1_two_layers_CNN.png \"CNN with two layers\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f08a56e801415af"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# create the cnn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.928621608Z",
     "start_time": "2024-01-04T05:41:33.925453283Z"
    }
   },
   "id": "b28caefe0488cb6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### compile the model \n",
    "Compile the model with Adam optimizer and binary cross entropy as loss function. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cc869f2e70dac45"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# compile the CNN "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.929001496Z",
     "start_time": "2024-01-04T05:41:33.925570783Z"
    }
   },
   "id": "eb2430dfd29e1c57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model with 128 epochs and 64 batch size "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bcf9ab22327a47e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# do it here"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.929104010Z",
     "start_time": "2024-01-04T05:41:33.925826026Z"
    }
   },
   "id": "58137563e9d32294"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the model and report the accuracy "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0866b441448ce0b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.929196604Z",
     "start_time": "2024-01-04T05:41:33.925947990Z"
    }
   },
   "id": "12b8b33ff18d26e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make prediction with the test set and use a threshold of 0.5 as boundaries decision between the classes. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38b37328ab11d9a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# do it here "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.929283555Z",
     "start_time": "2024-01-04T05:41:33.926063644Z"
    }
   },
   "id": "3d67ead4552d7561"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### show predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2773e4d28fb032d4"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def show_some_prediction(number_of_subplot, test_set, predictions, name_of_the_labels):\n",
    "    for i in range(number_of_subplot):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(test_set[i])\n",
    "        plt.title(f'{name_of_the_labels[predictions[i]]}')\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.929614755Z",
     "start_time": "2024-01-04T05:41:33.926176814Z"
    }
   },
   "id": "10717c3a5c56475d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# do it here "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.929712652Z",
     "start_time": "2024-01-04T05:41:33.926294008Z"
    }
   },
   "id": "1fac883027cf7ec5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### show metrics like confusion matrix or ROC curve or both (sklearn has already implemented all these stuff)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7a6e9042ab0d628"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T05:41:33.988218755Z",
     "start_time": "2024-01-04T05:41:33.968558774Z"
    }
   },
   "id": "2e6d76bf7d272bc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build another base CNN, but at point c add an extra hidden layer with 32 units of conv2d.  Repeat all the other steps. What happened to the accuracy of the model? Why?  \n",
    "\n",
    "![alt text](assignment_1_three_layers_CNN.png \"CNN with three layers\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5432f595cba4c81c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
